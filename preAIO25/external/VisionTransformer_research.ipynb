{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mylethidiem/zero-to-hero/blob/main/preAIO25/external/VisionTransformer_research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSpmFt9_S8qr"
      },
      "source": [
        "# **Vision Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "R6kBYpHvKfcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a70ee5-41b2-4eae-c0c3-53d0872e20c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision==0.16.0 in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (2.32.3)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (2.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->torchvision==0.16.0) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0->torchvision==0.16.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0->torchvision==0.16.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "!pip install torchvision==0.16.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "q9KlvJd2PBb-"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "SEED = 42\n",
        "set_seed(SEED)"
      ],
      "metadata": {
        "id": "Q0Ar5px1k7vF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d292a6-bf00-4d9e-b3cc-9c6fc0e4aed5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOAD DATA CIFAR10"
      ],
      "metadata": {
        "id": "s9lTN1HXsmeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hBa5tjThsy2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "\n",
        "# Why need transform??\n",
        "transform =  transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #?\n",
        "    ]\n",
        ")\n",
        "\n",
        "#download\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                             transform=transform, download=True)\n",
        "\n",
        "# split data train into val and train\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            transform=transform, download=True)\n",
        "train_subset_size = int(len(train_dataset) * 0.9)\n",
        "val_subset_size = len(train_dataset) - train_subset_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_subset_size,val_subset_size ])\n",
        "\n",
        "\n",
        "# create dataloader contain batches of data to train(mini-batch)\n",
        "train_loader = DataLoader(dataset=train_subset, batch_size=batch_size, shuffle=True, num_workers=2)#?\n",
        "val_loader = DataLoader(dataset=val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "print(f\"Train size: {len(train_subset)}\")\n",
        "print(f\"Validation size: {len(val_subset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "icHdsN5nlJrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26c19af-26ec-47fc-be36-946a1007bf5d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Train size: 45000\n",
            "Validation size: 5000\n",
            "Test size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model"
      ],
      "metadata": {
        "id": "lFY5ufFSJtgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A little research"
      ],
      "metadata": {
        "id": "gwOX5H6MVOtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch Embedding\n",
        "# usage, parameters?\n",
        "# Khong xai\n",
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, embed_dim=512, patch_size=16, image_size=224):\n",
        "    super().__init__()\n",
        "    # 1 layer conv2d\n",
        "    # image have 3 channels\n",
        "    # why we don't need a bias?\n",
        "    # why embed dim have that value?\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    # why we need reshape?\n",
        "    print(f\"x shape before reshape: {x.shape}\") # shape [*, width, grid, grid]\n",
        "    x = x.reshape(x.shape[0], x.shape[1], -1)\n",
        "    print(f\"x shape after reshape: {x.shape}\") # shape [*, width, grid ** 2]\n",
        "\n",
        "    # why need permute x shape?\n",
        "    x.permute(0, 2, 1)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Cz9KBCEPKLrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "patch_embedding = PatchEmbedding()\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "out = patch_embedding(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "k-n1nyEEV9LH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d55c3f-a2a0-4039-8275-b3e2953b1335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape before reshape: torch.Size([1, 512, 14, 14])\n",
            "x shape after reshape: torch.Size([1, 512, 196])\n",
            "torch.Size([1, 512, 196])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch Positional Embedding\n",
        "# usage, parameters?\n",
        "class PatchPositionalEmbedding(nn.Module):\n",
        "  def __init__(self, embed_dim=512, patch_size=16, image_size=224):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size, bias=False)\n",
        "    scale = embed_dim ** -0.5 #?\n",
        "    print(embed_dim)\n",
        "    data = scale * torch.randn((image_size // patch_size) ** 2, embed_dim)#?\n",
        "    self.positional_embedding = nn.Parameter(data)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x) # shape = [*, width, grid]\n",
        "    x = x.reshape(x.shape[0], x.shape[1], -1) # shape = [*, width, grid ** 2]\n",
        "    x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
        "\n",
        "    x = x + self.positional_embedding.to('cpu') #?\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "EGZg031dski5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patchpos_embedding = PatchPositionalEmbedding()\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "out = patchpos_embedding(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "h4lkPj-jJ16r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79b3b91a-e64b-4d4f-db81-5bc40b349f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n",
            "torch.Size([1, 196, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build model"
      ],
      "metadata": {
        "id": "6MPUYuahVY09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding [CLS] token\n",
        "class PatchPostionalEmbeddingCLSToken(nn.Module):\n",
        "  def __init__(self, embed_dim=512, patch_size=16, image_size=224):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size, bias=False)\n",
        "\n",
        "    scale = embed_dim ** -0.5\n",
        "\n",
        "    data_class_embedding = scale * torch.randn(embed_dim)\n",
        "    self.class_embedding = nn.Parameter(data_class_embedding)\n",
        "\n",
        "    #?\n",
        "    data_positional_embedding = scale * torch.randn((image_size // patch_size) ** 2 + 1, embed_dim)\n",
        "    self.positional_embedding = nn.Parameter(data_positional_embedding)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x) # shape = [*, width, grid, grid]\n",
        "    x = x.reshape(x.shape[0], x.shape[1], -1) # shape = [*, witdh, grid ** 2]\n",
        "    x = x.permute(0, 2, 1) # shape = [*, grid ** 2, width]\n",
        "\n",
        "    # expanding the CLS embedding?\n",
        "    cls_embs = self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device)\n",
        "    x = torch.cat([cls_embs, x], dim=1) #shape = [*, grid ** 2 + 1, width]\n",
        "\n",
        "    x = x + self.positional_embedding.to(x.dtype)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "glLZ_dxeuy9T"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ex\n",
        "patchpos_embedding = PatchPostionalEmbeddingCLSToken()\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "out = patchpos_embedding(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "4KtcrEyWEGmt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ae603d-ead2-48fd-aaaf-c6af91b5a32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 197, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class\n",
        "class ViTTransformerEncoder(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.norm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "    self.attn = nn.MultiheadAttention(\n",
        "        embed_dim=embed_dim,\n",
        "        num_heads=num_heads,\n",
        "        batch_first=True\n",
        "    )\n",
        "    self.norm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n",
        "    )\n",
        "    self.dropout_1 = nn.Dropout(p=dropout)\n",
        "    self.dropout_2 = nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # LayerNorm before Multi-Head Attention\n",
        "    norm_x1 = self.norm_1(x)\n",
        "    attn_output, _ = self.attn(norm_x1, norm_x1, norm_x1)\n",
        "    attn_output = self.dropout_1(attn_output)\n",
        "    x = x + attn_output # Residual connection\n",
        "\n",
        "    # LayerNorm before Feed-Forward Network\n",
        "    norm_x2 = self.norm_2(x)\n",
        "    ffn_output = self.ffn(norm_x2)\n",
        "    ffn_output = self.dropout_2(ffn_output)\n",
        "    x = x + ffn_output # Residual connection\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "FPta8jigF2Ez"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#?\n",
        "class VisionTransformerCLS(nn.Module):\n",
        "  def __init__(self,\n",
        "               image_size, embed_dim, num_heads, ff_dim,\n",
        "               dropout=0.1, num_classes=10, patch_size=16) -> None:\n",
        "    super().__init__()\n",
        "    self.embed_layer = PatchPostionalEmbeddingCLSToken(\n",
        "        image_size=image_size, embed_dim=embed_dim, patch_size=patch_size\n",
        "    )\n",
        "    self.transformer_layer = ViTTransformerEncoder(\n",
        "        embed_dim, num_heads, ff_dim, dropout\n",
        "    )\n",
        "    why_20 = 20\n",
        "    self.fc1 = nn.Linear(in_features=embed_dim, out_features=why_20)# why 20?\n",
        "    self.fc2 = nn.Linear(in_features=why_20, out_features=num_classes)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embed_layer(x)\n",
        "    #Corrected to single input\n",
        "    x = self.transformer_layer(x)\n",
        "    x = x[:, 0, :]\n",
        "\n",
        "    # Classification head\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x) # Optional activation for fc1\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "dkpgIkn-Ij3U"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ex\n",
        "image_size = 224\n",
        "embed_dim = 512\n",
        "num_heads = 4\n",
        "ff_dim = 128\n",
        "dropout = 0.1\n",
        "\n",
        "vit = VisionTransformerCLS(image_size, embed_dim, num_heads, ff_dim, dropout)"
      ],
      "metadata": {
        "id": "rfS1XWN48Ubq"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ex\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "out = vit(x) # call model to classify\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "AFGkmNjqNWrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d84b6a-3725-4765-b957-1f325d834ed6"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "lLL_RR6sNujn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize model value\n",
        "image_size = 224\n",
        "embed_dim = 512\n",
        "num_heads = 4\n",
        "ff_dim = 128\n",
        "dropout = 0.1\n",
        "learning_rate = 1e-5\n",
        "\n",
        "model = VisionTransformerCLS(\n",
        "    image_size, embed_dim, num_heads, ff_dim, dropout, num_classes=10\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "son_SW6iNsUt"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# train function\n",
        "def train_epoch(model, optimizer, criterion, train_dataloader, device, epoch=0, log_interval=50):\n",
        "  model.train()\n",
        "  total_acc, total_count = 0, 0 # total\n",
        "  losses = []\n",
        "  start_time = time.time()\n",
        "\n",
        "  for idx, (inputs, train_labels) in enumerate(train_dataloader):\n",
        "    inputs, train_labels = inputs.to(device), train_labels.to(device)\n",
        "\n",
        "    # reset gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward\n",
        "    predictions = model(inputs)\n",
        "\n",
        "    # loss\n",
        "    loss = criterion(predictions, train_labels)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # backward\n",
        "    loss.backward()\n",
        "\n",
        "    # update weight\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate the number of correct predictions in the current batch\n",
        "    #\n",
        "    total_acc += (predictions.argmax(1) == train_labels).sum().item()\n",
        "    # calculate train size\n",
        "    total_count += train_labels.size(0) # len(train_labels)\n",
        "\n",
        "    # print train for each epoch?...\n",
        "    # log_interval = num_epochs?\n",
        "    # idx =0,.. number of train loader(= train dataset / batch size) - 1\n",
        "    if idx % log_interval == 0 and idx > 0:\n",
        "      elapsed = time.time() - start_time\n",
        "      print(\n",
        "          \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "          \"| accuracy {:8.3f}\"\n",
        "          \"| device {}\".format(\n",
        "              epoch, idx, len(train_dataloader), total_acc / total_count, device\n",
        "          )\n",
        "      )\n",
        "      total_acc, total_count = 0, 0\n",
        "      start_time = time.time()\n",
        "  epoch_acc = total_acc / total_count # average accuracy loss\n",
        "  epoch_loss = sum(losses) / len(losses) # average train loss\n",
        "  return epoch_acc, epoch_loss\n"
      ],
      "metadata": {
        "id": "zwhBd4NTPVah"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#validation fucntion\n",
        "def validation_epoch(model, criterion, val_dataloader, device):\n",
        "  model.eval()\n",
        "  total_acc, total_count = 0, 0\n",
        "  losses = []\n",
        "\n",
        "  # does not need call update gradient\n",
        "  with torch.no_grad():\n",
        "    for idx, (inputs, val_labels) in enumerate(val_dataloader):\n",
        "      inputs, labels = inputs.to(device), val_labels.to(device)\n",
        "\n",
        "      predictions = model(inputs)\n",
        "\n",
        "      loss = criterion(predictions, val_labels)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      total_acc += (predictions.argmax(1) == val_labels).sum().item()\n",
        "      total_count += val_labels.size(0)\n",
        "\n",
        "  epoch_acc = total_acc / total_count\n",
        "  epoch_loss = sum(losses) / len(losses)\n",
        "  return epoch_acc, epoch_loss"
      ],
      "metadata": {
        "id": "14Wm2pWdRHI0"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AOdb5dxjT7e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, model_name, save_model_path, optimizer, criterion, dataloader, num_epochs, device):\n",
        "  \"\"\"\n",
        "  Usage: Train, validate model and save model\n",
        "  dataloader = [train_dataloader, val_loader]\n",
        "  \"\"\"\n",
        "  train_lost_list, train_acc_list = [], []\n",
        "  val_lost_list, val_acc_list = [], []\n",
        "  times = []\n",
        "  best_loss_eval = 100\n",
        "  train_loader, val_loader = dataloader[0], dataloader[1]\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    #train state\n",
        "    train_acc, train_lost = train_epoch(model, optimizer, criterion, train_loader, device, epoch, log_interval=int(len(train_loader) / 5))\n",
        "\n",
        "    #validate state\n",
        "    val_acc, val_lost = validation_epoch(model, criterion, val_loader, device)\n",
        "\n",
        "    # save the best model\n",
        "    if val_lost < best_loss_eval:\n",
        "      torch.save(model.state_dict(), save_model_path + f'/{model_name}.pt')\n",
        "\n",
        "    times.append(time.time() - epoch_start_time)\n",
        "\n",
        "    # append value\n",
        "    train_lost_list.append(train_lost)\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_lost_list.append(val_lost)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    # print result for each epoch\n",
        "    print(\"-\" * 59)\n",
        "    print(\n",
        "        \"| End of epoch {:3d} | Time: {:5.2f}s | Train Accuracy {:8.3f} \"\n",
        "        \"| Validate Accuracy {:8.3f} | Validate Loss {:8.3f} | Device {}\".format(\n",
        "            epoch + 1, time.time() - epoch_start_time, train_acc, train_lost, val_acc, val_lost, device\n",
        "        )\n",
        "    )\n",
        "    print(\"-\" * 59)\n",
        "  # Load best model de lam gi?\n",
        "  model.load_state_dict(torch.load(save_model_path + f'/{model_name}.pt'))\n",
        "  print(f\"train device: {device}\")\n",
        "  model.eval()\n",
        "  metrics = {\n",
        "      'train_accuracy' : train_acc_list,\n",
        "      'train_loss' : train_lost_list,\n",
        "      'val_accuracy' : val_acc_list,\n",
        "      'val_loss' : val_lost_list,\n",
        "      'time' : times\n",
        "  }\n",
        "  return model, metrics"
      ],
      "metadata": {
        "id": "-ZClHdmWRp1V"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#input out put cua bai toan la gi, co the cai tien, kho khan cua bai toan la gi,\n",
        "# co the cai tien bai toan nay hay khong?\n",
        "# training and save model\n",
        "num_epochs = 50\n",
        "save_model_path = './model'\n",
        "os.makedirs(save_model_path, exist_ok=True)\n",
        "model_name = 'model_vit_cifar10'\n",
        "\n",
        "dataloader = [train_loader, val_loader]\n",
        "print(device)\n",
        "model, metrics = train(\n",
        "    model, model_name, save_model_path, optimizer, criterion, dataloader, num_epochs, device\n",
        ")"
      ],
      "metadata": {
        "id": "nYs2llzcO5YZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "458cc3eb-ecde-43f1-a9b3-22fa3927d7ed"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "| epoch   0 |  2250/11250 batches | accuracy    0.434| device cuda\n",
            "| epoch   0 |  4500/11250 batches | accuracy    0.427| device cuda\n",
            "| epoch   0 |  6750/11250 batches | accuracy    0.425| device cuda\n",
            "| epoch   0 |  9000/11250 batches | accuracy    0.441| device cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-82555f5f3801>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m model, metrics = train(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-87-f27165d8abbe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, model_name, save_model_path, optimizer, criterion, dataloader, num_epochs, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#validate state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_lost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# save the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-5e5d9184d450>\u001b[0m in \u001b[0;36mvalidation_epoch\u001b[0;34m(model, criterion, val_dataloader, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "Djsf-Z59J4Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_result(num_epochs, train_accs, eval_accs, train_losses, eval_losses):\n",
        "  epochs = list(range(num_epochs))\n",
        "  fig, axs = plt.subplots(nrows= 1, ncols=2, figsize=(12, 6))\n",
        "\n",
        "  axs[0].plot(epochs, train_accs, label=\"Training Accuaracy\", color='green')\n",
        "  axs[0].plot(epochs, eval_accs, label=\"Evaluation Accuaracy\", color='orange')\n",
        "  axs[0].set(xlabel='Epoch', ylabel='Accuaracy')\n",
        "\n",
        "  axs[1].plot(epochs, train_losses, label=\"Training Loss\", color='green')\n",
        "  axs[1].plot(epochs, eval_losses, label=\"Evaluation Loss\", color='orange')\n",
        "  axs[1].set(xlabel='Epoch', ylabel='Loss')\n",
        "  plt.legend()\n"
      ],
      "metadata": {
        "id": "VCTJDfg5Patw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_result(\n",
        "    num_epochs,\n",
        "    metrics[\"train_accuracy\"],\n",
        "    metrics[\"valid_accuracy\"],\n",
        "    metrics[\"train_loss\"],\n",
        "    metrics[\"valid_loss\"]\n",
        ")"
      ],
      "metadata": {
        "id": "ORy9vwk1J3pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate by Test dataset"
      ],
      "metadata": {
        "id": "0ZeuB1htPkim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_target, test_predict = [], []\n",
        "model.load_state_dict(torch.load(save_model_path + f'/{model_name}.pt'))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in test_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    predictions = model()\n",
        "\n",
        "    test_target.append(inputs.cpu())\n",
        "    test_predict.append(predictions.cpu())\n",
        "test_target = torch.cat(test_target, dim=0)\n",
        "test_predict = torch.cat(test_predict, dim=0)\n",
        "\n",
        "test_acc = (torch.argmax(test_predict, dim=1)==test_target).sum().item() / len(test_target)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "yZhqeF9nPkAE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "d8c4ba8c-f46f-4666-a289-54fd89c55fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './model/model_vit_cifar10.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-14e7113fe6d0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'/{model_name}.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/model_vit_cifar10.pt'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}